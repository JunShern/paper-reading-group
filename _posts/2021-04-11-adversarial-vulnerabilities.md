---
layout: post
title: "Adversarial vulnerabilities of human decision-making."
date: 2021-04-11 23:00:00 +0800
slides: "https://docs.google.com/presentation/d/e/2PACX-1vT-7acUbwbFNo1co5d2jiVoNCqimlQuN0m2rPOlVQeoNRWpEHkEB6HtW9pHCzaxiDLaTOCcUW9EHWD5/"
insta: "https://www.instagram.com/explore/tags/paperreadinggroup017/"
reddit: 
twitter: 
---

<a href="https://www.pnas.org/content/pnas/117/46/29221.full.pdf" target="_blank">
Dezfouli, Amir, Richard Nock, and Peter Dayan. "Adversarial vulnerabilities of human decision-making." Proceedings of the National Academy of Sciences 117.46 (2020): 29221-29228.
</a>

The stated motivation of the authors is to study human psychology through adversarial perturbation, but lacking any psychology training myself, I am more focused on the potential of AI that can effectively model and manipulate human behavior.

"Machines manipulating humans" sounds ominous, but I think manipulative AI gets a bad rep from click-hungry recommendation systems that don't factor human welfare into their goals. In a much more agreeable light, consider that you could either have a "dumb" robot that everybody trips on and falls over, or a robot that accurately models and coordinates (=manipulates) the other actors in its environment toward a happy outcome for all. This is exactly the motivation for lots of human-robot interaction work, which this paper seems to be closely related to.

Careful followers of my recent trail of papers may notice that I've recently been quite taken by the idea of having AI mechanisms in the world that act as coordinators, coaches, facilitators to help us be our best selves. Thus I was very excited about the demonstrations carried out in this work: the tasks are simple proofs-of-concept, but the outcomes are quite indicative of real impact with real humans (which in my opinion makes this stand out from simulated multi-agent RL work)!

So I wonder, what would be a non-trivial real-world application of this framework? Are there useful applications simple enough for us to immediately apply this to? And thinking about the applications which are much more exciting (e.g. machine teaching, conflict resolution), what are the gaps we need to address to make this work there? I have a sense that 2021's RL algorithms should be able to take this much further in terms of higher-dimensional spaces, larger numbers of agents, less obvious reward schemes, and other contact interfaces (e.g. images, video, text, voice). There must be lots of things we can start trying, and I can't wait to start.